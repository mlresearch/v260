---
title: 'DCoT: Dual Chain-of-Thought Prompting for Large Multimodal Models'
booktitle: Proceedings of the 16th Asian Conference on Machine Learning
year: '2025'
volume: '260'
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: https://raw.githubusercontent.com/mlresearch/v260/main/assets/jia25b/jia25b.pdf
url: https://proceedings.mlr.press/v260/jia25b.html
openreview: 0saecDOdh2
abstract: Inference augmentation techniques such as Chain-of-Thought have already
  made their mark in Large Language Models (LLMs). However, transferring these advances
  to Large Multimodal Models (LMMs) presents greater challenges. Drawing inspiration
  from human cognitive processes, this paper proposes a plug-and-play Dual Chain-of-Thought
  strategy, a novel pipeline that combines visual and textual guidance to improve
  the performance of LMMs in complex multimodal tasks. The DCoT strategy uses a dual
  guidance mechanism to use bounding box markers to guide the modelâ€™s attention to
  the image region related to the query problem in the visual aspect, so as to achieve
  fine-grained image guidance, and in the text aspect, we propose a Fast In-Context
  Retrieval Framework (FICRF) dynamically and automatically obtains the most suitable
  examples from the well-built demonstration example cluster as context guidance according
  to the current problem. This bimodal approach that utilizes visual and textual guidance
  enhances the inference capabilities of LMMs. Extensive experiments on different
  LMMs and benchmark datasets have validated its effectiveness, opening up a new path
  in multimodal inference. Showcasing how the synergistic combination of visual and
  textual instructions can take the performance of these models to new heights, while
  demonstrating the potential of Chain-of-Thought and In-Context Learning as a superior
  alternative to the fine-tuning of LMMs.
layout: inproceedings
issn: 2640-3498
id: jia25b
tex_title: "{DCoT}: {D}ual Chain-of-Thought Prompting for Large Multimodal Models"
firstpage: 1064
lastpage: 1079
page: 1064-1079
order: 1064
cycles: false
bibtex_editor: Nguyen, Vu and Lin, Hsuan-Tien
editor:
- given: Vu
  family: Nguyen
- given: Hsuan-Tien
  family: Lin
bibtex_author: Jia, Zixi and Liu, Jiqiang and Li, Hexiao and Liu, Qinghua and Gao,
  Hongbin
author:
- given: Zixi
  family: Jia
- given: Jiqiang
  family: Liu
- given: Hexiao
  family: Li
- given: Qinghua
  family: Liu
- given: Hongbin
  family: Gao
date: 2025-01-14
address:
container-title: Proceedings of the 16th Asian Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 14
extras:
- label: Supplementary PDF
  link: https://raw.githubusercontent.com/mlresearch/v260/main/assets/assets/jia25b/jia25b-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
