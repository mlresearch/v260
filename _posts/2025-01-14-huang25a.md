---
title: 'Refining Visual Perception for Decoration Display: A Self-Enhanced Deep Captioning
  Model'
booktitle: Proceedings of the 16th Asian Conference on Machine Learning
year: '2025'
volume: '260'
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: https://raw.githubusercontent.com/mlresearch/v260/main/assets/huang25a/huang25a.pdf
url: https://proceedings.mlr.press/v260/huang25a.html
openreview: VPJne9waxR
abstract: Traditional decoration displays usually include renderings and corresponding
  descriptions to give users a deeper understanding and feeling. Nevertheless, describing
  massive renderings undoubtedly requires a lot of manpower. Thanks to the development
  of artificial intelligence, especially deep learning techniques, image captioning
  has been developed to automatically generate captions for given images. However,
  the defect of exploring “perceptive’’ words (e.g., bright, capacious, and comfortable,
  etc) is exposed when transferring existing captioning approaches to the decoration
  display task. To address this issue, in this paper, we propose a self-enhanced deep
  captioning model, which generates the captions with visual perception using the
  designed Self-Enhanced Transformer (SET). In detail, SET first pre-trains the scene-aware
  encoder, which employs the multi-task-based multi-modal transformer to enhance the
  perceptive semantics of the visual representations. Then, SET combines the pre-trained
  encoder with the transformer decoder for fine-tuning and designs a knowledge-enhanced
  module on the top of the decoder to adaptively fuse the decoded representations
  and retrieved language cues for making more suitable word prediction. In experiments,
  we first validate SET on the MS-COCO dataset, and we achieve at least 0.6 improvements
  on the CIDEr-D score. Furthermore, to address the effectiveness of SET on the decoration
  display task, we collect a new dataset called DecorationCap. We present a thorough
  empirical analysis to verify the generality of SET and find that SET surpasses other
  comparison methods with at least 6.8 improvements on the CIDEr-D score.
layout: inproceedings
issn: 2640-3498
id: huang25a
tex_title: "{Refining Visual Perception for Decoration Display}: {A} Self-Enhanced
  Deep Captioning Model"
firstpage: 527
lastpage: 542
page: 527-542
order: 527
cycles: false
bibtex_editor: Nguyen, Vu and Lin, Hsuan-Tien
editor:
- given: Vu
  family: Nguyen
- given: Hsuan-Tien
  family: Lin
bibtex_author: Huang, Longfei and Wu, Xiangyu and Wang, Jingyuan and Guo, Weili and
  Yang, Yang
author:
- given: Longfei
  family: Huang
- given: Xiangyu
  family: Wu
- given: Jingyuan
  family: Wang
- given: Weili
  family: Guo
- given: Yang
  family: Yang
date: 2025-01-14
address:
container-title: Proceedings of the 16th Asian Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 14
extras:
- label: Supplementary PDF
  link: https://raw.githubusercontent.com/mlresearch/v260/main/assets/assets/huang25a/huang25a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
