---
title: Analyzing Diffusion Models on Synthesizing Training Datasets
booktitle: Proceedings of the 16th Asian Conference on Machine Learning
year: '2025'
volume: '260'
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: https://raw.githubusercontent.com/mlresearch/v260/main/assets/yamaguchi25a/yamaguchi25a.pdf
url: https://proceedings.mlr.press/v260/yamaguchi25a.html
openreview: x2PY6GP9GJ
abstract: Synthetic samples from diffusion models are promising for training discriminative
  models as replications or augmentations of real training datasets. However, we found
  that the synthetic datasets degrade classification performance over real datasets
  when using the same dataset size. This means that the synthetic samples from modern
  diffusion models are less informative for training discriminative tasks. This paper
  investigates the gap between synthetic and real samples by analyzing the synthetic
  samples reconstructed from real samples through the noising (diffusion) and denoising
  (reverse) process of diffusion models. By varying the time steps starting the reverse
  process in the reconstruction, we can control the trade-off between the information
  in the original real data and the information produced by diffusion models. Through
  assessing the reconstructed samples and the trained models, we found that the synthetic
  samples are concentrated in modes of the training data distribution as the reverse
  step increases, and thus, they have difficulty covering the outer edges of the distribution.
  On the contrary, we found that these synthetic samples yield significant improvements
  in the data augmentation setting where both real and synthetic samples are used,
  indicating that the samples around modes are useful as interpolation for learning
  classification boundaries. These findings suggest that modern diffusion models are
  currently insufficient to replicate the real training dataset in the same dataset
  size but are suitable for interpolating the real training samples as the augment
  datasets.
layout: inproceedings
issn: 2640-3498
id: yamaguchi25a
tex_title: Analyzing Diffusion Models on Synthesizing Training Datasets
firstpage: 335
lastpage: 350
page: 335-350
order: 335
cycles: false
bibtex_editor: Nguyen, Vu and Lin, Hsuan-Tien
editor:
- given: Vu
  family: Nguyen
- given: Hsuan-Tien
  family: Lin
bibtex_author: Yamaguchi, Shin'ya
author:
- given: Shinâ€™ya
  family: Yamaguchi
date: 2025-01-14
address:
container-title: Proceedings of the 16th Asian Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 14
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
