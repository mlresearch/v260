---
title: A More Efficient Inference Model for Multimodal Emotion Recognition
booktitle: Proceedings of the 16th Asian Conference on Machine Learning
year: '2025'
volume: '260'
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: https://raw.githubusercontent.com/mlresearch/v260/main/assets/jia25a/jia25a.pdf
url: https://proceedings.mlr.press/v260/jia25a.html
openreview: vA0IDtOARE
abstract: With the widespread adoption of the Internet and mobile Internet, an increasing
  number of individuals are expressing their emotions on short-video platforms. Contemporary
  multimodal emotion analysis technologies facilitate a more comprehensive recognition
  and understanding of emotions through the analysis of various data sources including
  text, facial expressions, audio, hand gestures, among others. Consequently, the
  significance of sentiment analysis is becoming increasingly pronounced. However,
  existing research indicates that most emotion analysis techniques are not sufficiently
  rapid and efficient in light of the exponential proliferation of short video content.
  In addition, most sentiment analysis models demonstrate significant differences
  in the contribution of each modality, with text and visual modalities often exerting
  a greater influence than audio modes. Furthermore, in the pursuit of heightened
  accuracy, certain models are designed to be exceedingly complex, while others prioritize
  swift reasoning at the expense of accuracy. This paper proposes a more efficient
  multimodal sentiment analysis model, presenting three distinct advantages. Firstly,
  residual-free connectivity modules capable of extracting 3-D attentional weights
  are proposed to process visual modal features, maintaining accuracy while improving
  inference efficiency. Secondly, adoption of multi-scale hierarchical context aggregation
  (aggregation followed by interaction) for audio modality to capture coarse- and
  fine-grained audio contextual information through multilevel aggregation, thereby
  enriching audio modality features and minimizing disparities between modalitiesâ€™
  contributions. Finally, attainment of a superior balance between accuracy and speed,
  thereby enhancing adaptability to the fast-paced short video environment and meeting
  the burgeoning demand for video content processing.
layout: inproceedings
issn: 2640-3498
id: jia25a
tex_title: A More Efficient Inference Model for Multimodal Emotion Recognition
firstpage: 49
lastpage: 63
page: 49-63
order: 49
cycles: false
bibtex_editor: Nguyen, Vu and Lin, Hsuan-Tien
editor:
- given: Vu
  family: Nguyen
- given: Hsuan-Tien
  family: Lin
bibtex_author: Jia, Liang and Tan, Jin and Qi, Lijin and Lin, Mingwen
author:
- given: Liang
  family: Jia
- given: Jin
  family: Tan
- given: Lijin
  family: Qi
- given: Mingwen
  family: Lin
date: 2025-01-14
address:
container-title: Proceedings of the 16th Asian Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 14
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
