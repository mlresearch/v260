---
title: Vision Transformer with High Spatial Structure Sensitivity
booktitle: Proceedings of the 16th Asian Conference on Machine Learning
year: '2025'
volume: '260'
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: https://raw.githubusercontent.com/mlresearch/v260/main/assets/xu25b/xu25b.pdf
url: https://proceedings.mlr.press/v260/xu25b.html
openreview: DJJC69Uc1I
abstract: "Self-attention operation, the core operation of the vision transformer
  (VT), is position-independent. Therefore, VT uses positional embedding to encode
  spatial information. However, we found that the role of positional encoding is very
  limited, and VT is insensitive to spatial structure. We demonstrated a significant
  sensitivity gap to random block shuffling and masking between VT and convolutional
  neural network (CNN), which indicates that VT does not learn the spatial structure
  of the target well and focuses too much on small-scale detail features.\r We argue
  that self-attention should use position-dependent operations to encode spatial information
  instead of relying on positional embedding. We replace the linear projection of
  self-attention with convolution operation and use regular receptive field for each
  feature point, which significantly increases VTâ€™s sensitivity to spatial structure
  without sacrificing performance."
layout: inproceedings
issn: 2640-3498
id: xu25b
tex_title: Vision Transformer with High Spatial Structure Sensitivity
firstpage: 735
lastpage: 749
page: 735-749
order: 735
cycles: false
bibtex_editor: Nguyen, Vu and Lin, Hsuan-Tien
editor:
- given: Vu
  family: Nguyen
- given: Hsuan-Tien
  family: Lin
bibtex_author: Xu, Zhiwei
author:
- given: Zhiwei
  family: Xu
date: 2025-01-14
address:
container-title: Proceedings of the 16th Asian Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 14
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
