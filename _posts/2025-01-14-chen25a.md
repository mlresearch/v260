---
title: 'Motion meets Attention: Video Motion Prompts'
booktitle: Proceedings of the 16th Asian Conference on Machine Learning
year: '2025'
volume: '260'
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: https://raw.githubusercontent.com/mlresearch/v260/main/assets/chen25a/chen25a.pdf
url: https://proceedings.mlr.press/v260/chen25a.html
software: https://github.com/q1xiangchen/VMPs
openreview: nIDAT99Vhb
abstract: Videos contain rich spatio-temporal information. Traditional methods for
  extracting motion, used in tasks such as action recognition, often rely on visual
  contents rather than precise motion features. This phenomenon is referred to as
  ’blind motion extraction’ behavior, which proves inefficient in capturing motions
  of interest due to a lack of motion-guided cues. Recently, attention mechanisms
  have enhanced many computer vision tasks by effectively highlighting salient visual
  areas. Inspired by this, we propose a modified Sigmoid function with learnable slope
  and shift parameters as an attention mechanism to modulate motion signals from frame
  differencing maps. This approach generates a sequence of attention maps that enhance
  the processing of motion-related video content. To ensure temporal continuity and
  smoothness of the attention maps, we apply pair-wise temporal attention variation
  regularization to remove unwanted motions (e.g., noise) while preserving important
  ones. We then perform Hadamard product between each pair of attention maps and the
  original video frames to highlight the evolving motions of interest over time. These
  highlighted motions, termed video motion prompts, are subsequently used as inputs
  to the model instead of the original video frames. We formalize this process as
  a motion prompt layer and incorporate the regularization term into the loss function
  to learn better motion prompts. This layer serves as an adapter between the model
  and the video data, bridging the gap between traditional ’blind motion extraction’
  and the extraction of relevant motions of interest. We show that our lightweight,
  plug-and-play motion prompt layer seamlessly integrates into models like SlowFast,
  X3D, and TimeSformer, enhancing performance on benchmarks such as FineGym and MPII
  Cooking 2.
layout: inproceedings
issn: 2640-3498
id: chen25a
tex_title: "{Motion meets Attention}: {V}ideo Motion Prompts"
firstpage: 591
lastpage: 606
page: 591-606
order: 591
cycles: false
bibtex_editor: Nguyen, Vu and Lin, Hsuan-Tien
editor:
- given: Vu
  family: Nguyen
- given: Hsuan-Tien
  family: Lin
bibtex_author: Chen, Qixiang and Wang, Lei and Koniusz, Piotr and Gedeon, Tom
author:
- given: Qixiang
  family: Chen
- given: Lei
  family: Wang
- given: Piotr
  family: Koniusz
- given: Tom
  family: Gedeon
date: 2025-01-14
address:
container-title: Proceedings of the 16th Asian Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 14
extras:
- label: Supplementary PDF
  link: https://raw.githubusercontent.com/mlresearch/v260/main/assets/assets/chen25a/chen25a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
