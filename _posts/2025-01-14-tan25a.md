---
title: Adapting the Attention of Cloud-Based Recognition Model to Client-Side Images
  without Local Re-Training
booktitle: Proceedings of the 16th Asian Conference on Machine Learning
year: '2025'
volume: '260'
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
pdf: https://raw.githubusercontent.com/mlresearch/v260/main/assets/tan25a/tan25a.pdf
url: https://proceedings.mlr.press/v260/tan25a.html
software: https://github.com/mikudehuane/ICIIA
openreview: i5YqAtOGiD
abstract: The mainstream workflow of image recognition applications is first training
  one global model on the cloud for a wide range of classes and then serving numerous
  clients. Images uploaded by each client typically come from a small subset of classes.
  From the cloud-client discrepancy on the range of image classes, the recognition
  model is desired to have strong adaptiveness, intuitively by focusing on each client’s
  local dynamic class subset, while incurring negligible overhead. In this work, we
  propose to plug a new intra-client and inter-image attention (ICIIA) module into
  existing backbone recognition models, requiring only one-time cloud-based training
  to be client-adaptive. In particular, given an image to be recognized from a certain
  client, ICIIA introduces multi-head self-attention to retrieve relevant images from
  the client’s local images, thereby calibrating the focus and the recognition result.
  We further identify the bottleneck of ICIIA’s overhead being in linear projection,
  propose to group and shuffle the features before the projections, and allow increasing
  the number of feature groups to dramatically improve efficiency without scarifying
  much accuracy. We extensively evaluate ICIIA and compare its performance against
  several baselines, demonstrating effectiveness and efficiency. Specifically, for
  a partitioned version of ImageNet-1K with the backbone models of MobileNetV3-L and
  Swin-B, ICIIA improves the classification accuracy to 83.37% (+8.11%) and 88.86%
  (+5.28%), while adding only 1.62% and 0.02% of FLOPs, respectively. Source code
  is available in the supplementary materials.
layout: inproceedings
issn: 2640-3498
id: tan25a
tex_title: Adapting the Attention of Cloud-Based Recognition Model to Client-Side
  Images without Local Re-Training
firstpage: 223
lastpage: 238
page: 223-238
order: 223
cycles: false
bibtex_editor: Nguyen, Vu and Lin, Hsuan-Tien
editor:
- given: Vu
  family: Nguyen
- given: Hsuan-Tien
  family: Lin
bibtex_author: Tan, Yangwenjian and Yan, Yikai and Niu, Chaoyue
author:
- given: Yangwenjian
  family: Tan
- given: Yikai
  family: Yan
- given: Chaoyue
  family: Niu
date: 2025-01-14
address:
container-title: Proceedings of the 16th Asian Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 14
extras:
- label: Supplementary PDF
  link: https://raw.githubusercontent.com/mlresearch/v260/main/assets/assets/tan25a/tan25a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
